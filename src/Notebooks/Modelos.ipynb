{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS USADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.load('./data_train_processed/x_train.npy')\n",
    "y_train = np.load('./data_train_processed/y_train.npy')\n",
    "x_test = np.load('./data_test_processed/x_test.npy')\n",
    "y_test = np.load('./data_test_processed/y_test.npy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESIÓN LOGÍSTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas Regresión Logística:\n",
      "Precisión: 0.85\n",
      "Recall: 0.83\n",
      "F1-score: 0.84\n",
      "Accuracy: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas Regresión Logística:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas Decision Tree:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = SVC()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas SVC:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas KNN:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDON FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas Random Forest:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas de clasificación:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convertir las etiquetas de clase a valores numéricos enteros ya que \n",
    "# XGBoost requiere que las etiquetas de clase sean valores numéricos enteros en lugar de cadenas de texto\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(x_train, y_train_encoded)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(\"Métricas XGboost:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas ADAboost:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Métricas MLP:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVOLUCIONALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Asegurarse de que los datos de entrada tengan la forma adecuada\n",
    "# Agregamos una dimensión adicional para representar el canal de color de las imágenes (en este caso, 1 para imágenes en escala de grises).\n",
    "# -1 se utiliza para que el tamaño de la primera dimensión se ajuste automáticamente según el tamaño original del conjunto de datos\n",
    "x_train = x_train.reshape(-1, 32, 32, 1)\n",
    "x_test = x_test.reshape(-1, 32, 32, 1)\n",
    "\n",
    "# Obtener el número de clases\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Convertir las etiquetas de clase a números enteros\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Codificar los datos de destino en one-hot\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predecir con el modelo\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calcular las métricas de clasificación\n",
    "precision = precision_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Imprimir las métricas de clasificación\n",
    "print(\"Métricas de Redes convolucionales:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Asegurarse de que los datos de entrada tengan la forma adecuada\n",
    "# Agregamos una dimensión adicional para representar el canal de color de las imágenes (en este caso, 1 para imágenes en escala de grises).\n",
    "# -1 se utiliza para que el tamaño de la primera dimensión se ajuste automáticamente según el tamaño original del conjunto de datos\n",
    "x_train = x_train.reshape(-1, 32, 32, 1)\n",
    "x_test = x_test.reshape(-1, 32, 32, 1)\n",
    "\n",
    "# Obtener el número de clases\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Convertir las etiquetas de clase a números enteros\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Codificar los datos de destino en one-hot\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))  # Capa Dropout para aplicar regularización por Dropout\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Capa Dropout para aplicar regularización por Dropout\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Se añaden dos capas Dropout desdepués de las capas convolucionales y densas que desactivan un porcentaje d\n",
    "# de las neuronas en el entrenamiento\n",
    "\n",
    "\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predecir con el modelo\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calcular las métricas de clasificación\n",
    "precision = precision_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Imprimir las métricas de clasificación\n",
    "print(\"Métricas de Redes convolucionales con Dropout:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Asegurarse de que los datos de entrada tengan la forma adecuada\n",
    "# Agregamos una dimensión adicional para representar el canal de color de las imágenes (en este caso, 1 para imágenes en escala de grises).\n",
    "# -1 se utiliza para que el tamaño de la primera dimensión se ajuste automáticamente según el tamaño original del conjunto de datos\n",
    "x_train = x_train.reshape(-1, 32, 32, 1)\n",
    "x_test = x_test.reshape(-1, 32, 32, 1)\n",
    "\n",
    "# Obtener el número de clases\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Convertir las etiquetas de clase a números enteros\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Codificar los datos de destino en one-hot\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Realizamos un modelo completamente distinto, con capas más sencillas (sin aplicar redes convolucionales) pero añadiendo la capa de BatchNormalization para ver como actua\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predecir con el modelo\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calcular las métricas de clasificación\n",
    "precision = precision_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Imprimir las métricas de clasificación\n",
    "print(\"Métricas de Redes convolucionales:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO DEFINITIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Asegurarse de que los datos de entrada tengan la forma adecuada\n",
    "# Agregamos una dimensión adicional para representar el canal de color de las imágenes (en este caso, 1 para imágenes en escala de grises).\n",
    "# -1 se utiliza para que el tamaño de la primera dimensión se ajuste automáticamente según el tamaño original del conjunto de datos\n",
    "x_train = x_train.reshape(-1, 32, 32, 1)\n",
    "x_test = x_test.reshape(-1, 32, 32, 1)\n",
    "\n",
    "# Obtener el número de clases\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Convertir las etiquetas de clase a números enteros\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Codificar los datos de destino en one-hot\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Crear el modelo\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Capa Dropout para aplicar regularización por Dropout\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Aplicamos Batch Normalization después de aplicar una covolución\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predecir con el modelo\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calcular las métricas de clasificación\n",
    "precision = precision_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "recall = recall_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "f1 = f1_score(np.argmax(y_test, axis=1), y_pred, average='weighted')\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "# Imprimir las métricas de clasificación\n",
    "print(\"Métricas de Redes convolucionales:\")\n",
    "print(\"Precisión: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-score: {:.2f}\".format(f1))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Añadimos dos capas de Batch Normalization, después de las capas convolucionales, aumentando así ligeramente la complejidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
